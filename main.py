import os
import pandas as pd
import numpy as np
from keras.layers.core import Dense, Activation, Dropout
from keras.layers.recurrent import LSTM
from keras.models import Sequential
np.random.seed(7)

def normalize(dcol):    
    min,max=dcol.values.min(),dcol.values.max()    
    vals = pd.DataFrame({'min': min*np.ones((len(dcol.index))), 'max': max*np.ones((len(dcol.index)))})
    dcol = (dcol.values-vals.loc[:,"min"])/(vals.loc[:,"max"]-vals.loc[:,"min"]) 
    
    return dcol

def runModel(X,Y):
    
    # create model
    model = Sequential()
    model.add(Dense(16, input_dim=7, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(8, activation='relu'))
    model.add(Dropout(0.2))
    model.add(Dense(1, activation='sigmoid'))

    #Compile model with loss function
    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

    # Fit the model
    model.fit(X, Y, epochs=1000, batch_size=10, verbose=0)

    # evaluate the model
    scores = model.evaluate(X, Y)
    print("\n%s: %.2f%%" % (model.metrics_names[1], scores[1]*100))
    
    return model

def cleanData(df,train=0):
    
    #Remove unneeded data
    df=df.drop(columns=['Name', 'Ticket','Cabin'])
    
    #Set NaNs to zero
    df=df.fillna(0)
    
    #Convert 'Sex' to numeric`````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````````
    for row in range(len(df.index)):
        if df.loc[row,"Sex"]=="female":
            df.loc[row,"Sex"]=1
        elif df.loc[row,"Sex"]=="male":
            df.loc[row,"Sex"]=0
    
    #Alter "Embarked" to 1,2,3
    for row in range(len(df.index)):
        if df.loc[row,"Embarked"]=="C":
            df.loc[row,"Embarked"]=0
        elif df.loc[row,"Embarked"]=="Q":
            df.loc[row,"Embarked"]=0.5
        elif df.loc[row,"Embarked"]=="S":
            df.loc[row,"Embarked"]=1     
    
    #Normalize data
    df.loc[:,"Pclass"]=normalize(df.loc[:,"Pclass"])
    df.loc[:,"Age"]=normalize(df.loc[:,"Age"])
    df.loc[:,"Fare"]=normalize(df.loc[:,"Fare"])
    df.loc[:,"Parch"]=normalize(df.loc[:,"Parch"])
    df.loc[:,"SibSp"]=normalize(df.loc[:,"SibSp"])
    
    #Split data
    Pid = df.loc[:,"PassengerId"]
    X = df.loc[:,"Pclass":"Embarked"]
    if (train==1):
        Y = df.loc[:,"Survived"]
    elif(train==0):
        Y=0
        
    return Pid,X,Y

#Read/clean test data
df = pd.read_csv('train.csv')
Pid,X,Y = cleanData(df,1)

#Call model
model = runModel(X.values,Y.values)

#Read/clean test data
df2 = pd.read_csv('test.csv')
Pid2,X2,_ = cleanData(df2)

#Calculate predictions
predictions = model.predict(X2.values)

#Round predictions
rounded = [round(x[0]) for x in predictions]

#Print result
with open('submission.csv', 'w') as fid:
    fid.write("PassengerId,Survived\n")
    for i in range(len(predictions)):
        fid.write(str(Pid2.loc[i]) +','+ str(int(rounded[i])) +'\n')



